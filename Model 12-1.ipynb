{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Conv1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 3)\n",
      "(19579,)\n",
      "(19579, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "print(df.shape)\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in df.author])\n",
    "print(y.shape)\n",
    "y = to_categorical(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate punctuation from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process ,  however ,  afforded me no mean...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box ,  from ...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else ,  not even gold ,  the S...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process ,  however ,  afforded me no mean...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box ,  from ...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else ,  not even gold ,  the S...    HPL   \n",
       "\n",
       "   author_num  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           2  \n",
       "4           1  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process the text outside of the create_docs function\n",
    "#df['text'] = df['text'].apply(preprocess)\n",
    "# convert author labels into numerical variables\n",
    "df['author_num'] = df.author.map({'EAP':0, 'HPL':1, 'MWS':2})\n",
    "# Check conversion for first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove lover frequency words (<=2)\n",
    "Remvoe the words that don't appear more than twice. Something I could try is to keep significant words even if they have a low frequency. These words could be unique to authors, like the word \"Cthulhu\" could be unique to H.P. Lovecraft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut down the length of the sentences in the train dataframe to a 500 character limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#accuracy is very close even when sentences are cut short.\n",
    "train_df = df\n",
    "train_df = train_df.rename(columns={'text':'original_text'})\n",
    "train_df['text'] = train_df['original_text'].str[:700]\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "#train_df.head()\n",
    "\n",
    "min_count = 2\n",
    "docs = train_df\n",
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 500\n",
    "#pad the documents to a max length of 500\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model with an embedding input layer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.max(docs)+1\n",
    "embedding_dims = 20\n",
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    #model.add(Conv1D(128, 5, activation='relu')) \n",
    "    #MaxPooling2D(5)\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Early Stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "def fit(num_epochs):\n",
    "    epochs = num_epochs\n",
    "    \n",
    "    print(\"x_train: \", x_test.shape)\n",
    "    print(\"y_train: \", y_train.shape)\n",
    "    model = create_model()\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     batch_size=16,\n",
    "                     validation_data=(x_test, y_test),\n",
    "                     epochs=epochs,\n",
    "                     callbacks=[EarlyStopping(patience=2, monitor='val_loss')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (3916, 500)\n",
      "y_train:  (15663, 3)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/30\n",
      "15663/15663 [==============================] - 6s - loss: 1.0795 - acc: 0.4068 - val_loss: 1.0682 - val_acc: 0.3989\n",
      "Epoch 2/30\n",
      "15663/15663 [==============================] - 6s - loss: 1.0223 - acc: 0.4689 - val_loss: 0.9863 - val_acc: 0.5260\n",
      "Epoch 3/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.8912 - acc: 0.6603 - val_loss: 0.8703 - val_acc: 0.6430\n",
      "Epoch 4/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.7385 - acc: 0.7893 - val_loss: 0.7606 - val_acc: 0.7561\n",
      "Epoch 5/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.6051 - acc: 0.8502 - val_loss: 0.6753 - val_acc: 0.7646\n",
      "Epoch 6/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.4971 - acc: 0.8830 - val_loss: 0.6120 - val_acc: 0.7860\n",
      "Epoch 7/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.4099 - acc: 0.9083 - val_loss: 0.5682 - val_acc: 0.7926\n",
      "Epoch 8/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.3384 - acc: 0.9296 - val_loss: 0.5167 - val_acc: 0.8243\n",
      "Epoch 9/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.2802 - acc: 0.9452 - val_loss: 0.4843 - val_acc: 0.8297\n",
      "Epoch 10/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.2314 - acc: 0.9572 - val_loss: 0.4619 - val_acc: 0.8281\n",
      "Epoch 11/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.1919 - acc: 0.9676 - val_loss: 0.4391 - val_acc: 0.8363\n",
      "Epoch 12/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.1588 - acc: 0.9735 - val_loss: 0.4209 - val_acc: 0.8440\n",
      "Epoch 13/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.1327 - acc: 0.9789 - val_loss: 0.4006 - val_acc: 0.8511\n",
      "Epoch 14/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.1093 - acc: 0.9845 - val_loss: 0.3898 - val_acc: 0.8491\n",
      "Epoch 15/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0912 - acc: 0.9881 - val_loss: 0.3813 - val_acc: 0.8509\n",
      "Epoch 16/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0761 - acc: 0.9899 - val_loss: 0.3691 - val_acc: 0.8557\n",
      "Epoch 17/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0633 - acc: 0.9931 - val_loss: 0.3706 - val_acc: 0.8534\n",
      "Epoch 18/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0532 - acc: 0.9941 - val_loss: 0.3591 - val_acc: 0.8550\n",
      "Epoch 19/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0441 - acc: 0.9957 - val_loss: 0.3583 - val_acc: 0.8570\n",
      "Epoch 20/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0370 - acc: 0.9962 - val_loss: 0.3576 - val_acc: 0.8570\n",
      "Epoch 21/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0313 - acc: 0.9973 - val_loss: 0.3619 - val_acc: 0.8573\n",
      "Epoch 22/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0266 - acc: 0.9975 - val_loss: 0.3520 - val_acc: 0.8601\n",
      "Epoch 23/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0222 - acc: 0.9980 - val_loss: 0.3513 - val_acc: 0.8593\n",
      "Epoch 24/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0187 - acc: 0.9985 - val_loss: 0.3597 - val_acc: 0.8601\n",
      "Epoch 25/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0158 - acc: 0.9987 - val_loss: 0.3766 - val_acc: 0.8529\n",
      "Epoch 26/30\n",
      "15663/15663 [==============================] - 6s - loss: 0.0134 - acc: 0.9991 - val_loss: 0.3618 - val_acc: 0.8580\n"
     ]
    }
   ],
   "source": [
    "model = fit(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy here is about 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_74 (Embedding)     (None, None, 20)          5141740   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_59  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 5,141,803\n",
      "Trainable params: 5,141,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Kaggle Submission #1:\n",
    "###### Model 1 scored a 0.36295 which is still the best score out of my first three submissions.\n",
    "\n",
    "###### This was a simple model, the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'd like to see how adding a convolutional layer to the network would affect its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7424/8392 [=========================>....] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n",
    "\n",
    "result = pd.read_csv('sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    result[a] = y[:, i]\n",
    "result.to_csv('my_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "This is based off of the first model, but here I will use sigmoid over softmax.\n",
    "Note: using optimizer 'sgd' over 'adam' will get a max score of 40%. It plateus pretty quickly..\n",
    "\n",
    "Using sigmoid gives an accuracy of about 62%. The tests are also much shorter and stop early. It seems like adding more layers makes it less accurate. \n",
    "\n",
    "I've noticed that the more layers I add on to this model, the less accurate it becomes. The only layer that has added anything was the Conv1D layer. Adding the layer speed up the test time since it doesn't run through all the epochs, it stops much earlier than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "def one_conv_model(embedding_dims=20, optimizer='rmsprop'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(Conv1D(128, 5, activation='relu')) #added a Conv1D layer\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    #model.add(Dense(3, activation='sigmoid'))\n",
    "    #model.add(Dropout(1))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the batch size is changed from 16 to 128, the number of epochs is reduced by 3/4.\n",
    "The accuracy of this model is about 85% When batch size = 128, the number of epochs is cut by two-thirds, and the test time is 1 second. The accuary is slightly worse than the model with a batch size of 16. I haven't achieved a higher score than 86% with these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_128(num_epochs):\n",
    "    epochs = num_epochs  \n",
    "    x_train2, x_test2, y_train2, y_test2 = train_test_split(docs, y, test_size=0.2)\n",
    "    model = one_conv_model()\n",
    "    hist = model.fit(x_train2, y_train2,\n",
    "                     batch_size=128,\n",
    "                     validation_data=(x_test2, y_test2),\n",
    "                     epochs=epochs,\n",
    "                     callbacks=[EarlyStopping(patience=2, monitor='val_loss')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 1s - loss: 1.0813 - acc: 0.4069 - val_loss: 1.0680 - val_acc: 0.3984\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.9954 - acc: 0.5069 - val_loss: 0.9348 - val_acc: 0.5804\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.7810 - acc: 0.6874 - val_loss: 0.7624 - val_acc: 0.6752\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.5895 - acc: 0.7829 - val_loss: 0.6389 - val_acc: 0.7428\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.4247 - acc: 0.8556 - val_loss: 0.5188 - val_acc: 0.7952\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.2842 - acc: 0.9156 - val_loss: 0.4327 - val_acc: 0.8271\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.1895 - acc: 0.9461 - val_loss: 0.3929 - val_acc: 0.8391\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.1317 - acc: 0.9631 - val_loss: 0.3714 - val_acc: 0.8547\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.0945 - acc: 0.9753 - val_loss: 0.3665 - val_acc: 0.8654\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.0701 - acc: 0.9817 - val_loss: 0.3762 - val_acc: 0.8647\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.0530 - acc: 0.9867 - val_loss: 0.3902 - val_acc: 0.8647\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 1s - loss: 0.0408 - acc: 0.9890 - val_loss: 0.4012 - val_acc: 0.8647\n"
     ]
    }
   ],
   "source": [
    "model2 = fit_128(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 20)          5141200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         12928     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 5,154,515\n",
      "Trainable params: 5,154,515\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7584/8392 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model2.predict_proba(docs)\n",
    "\n",
    "result = pd.read_csv('sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    result[a] = y[:, i]\n",
    "result.to_csv('model_2_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Kaggle Submission #3:\n",
    "#### Model 2 scored lower than Model 1, which didn't surprise me. I've been seeing that adding more to these models is actually making it worse. \n",
    "\n",
    "##### Model 1: 0.36295 \n",
    "##### Model 2: 0.42768"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
